{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEPvtK1IBEjw"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfJsnJg66G5q"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbsjNRGv6d3c",
    "outputId": "288cc73d-c0d6-4ae4-91b8-58c01525f179"
   },
   "outputs": [],
   "source": [
    "data_root = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\KohanovDiploma\\\\BraTS\\\\MICCAI_BraTS2020_TrainingData\\\\'\n",
    "val_root = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\KohanovDiploma\\\\BraTS\\\\MICCAI_BraTS2020_ValidationData\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdZYY2056mQ9"
   },
   "outputs": [],
   "source": [
    "def list_of_images(data_root, modal, folder_start=0, folder_finish=len(os.listdir(data_root))):\n",
    "    images = []\n",
    "    for fold in tqdm(os.listdir(data_root)[folder_start:folder_finish]):\n",
    "        if os.path.isdir(data_root + fold):\n",
    "            for item in os.listdir(data_root + fold + f'\\\\{modal}\\\\'):\n",
    "                images.append(data_root + fold + f'\\\\{modal}\\\\' + item)\n",
    "        else:\n",
    "            continue\n",
    "    return images\n",
    "\n",
    "def create_dataset_paths(images, masks):\n",
    "    images = np.array(images)\n",
    "    masks = np.array(masks)\n",
    "    return np.c_[images, masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahKptqrL8SuK",
    "outputId": "5fe81a35-98cc-4504-b537-359e2dc1dbc6"
   },
   "outputs": [],
   "source": [
    "images = list_of_images(data_root, 't1ce')\n",
    "masks = list_of_images(data_root, 'seg')\n",
    "\n",
    "train_paths = create_dataset_paths(images, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Evr_MK0GGSHb",
    "outputId": "1ea9a84e-8df0-4d3f-ef5b-0762f97b5498"
   },
   "outputs": [],
   "source": [
    "images_val = list_of_images(val_root, 't1ce')\n",
    "masks_val = list_of_images(val_root, 'seg')\n",
    "\n",
    "val_paths = create_dataset_paths(images_val, masks_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvYhIQfr_-q0"
   },
   "source": [
    "# Pytorch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDX6PVSg7Hvq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.files = files\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        image = cv2.imread(self.files[index][0])\n",
    "        \n",
    "        mask = cv2.imread(self.files[index][1])\n",
    "       \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            \n",
    "            image_transformed = transformed[\"image\"]\n",
    "            image_transformed = image_transformed/255\n",
    "            \n",
    "            mask_transformed = transformed[\"mask\"]\n",
    "            mask_transformed = mask_transformed/255\n",
    "            \n",
    "        return image_transformed, mask_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUlJFqwz8FA0",
    "outputId": "5efa73be-fca0-41b8-ce7b-f0bf2b9ea2a7"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256,256,cv2.INTER_NEAREST),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "    \n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256,256,cv2.INTER_NEAREST),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "train_dataset = BratsDataset(train_paths, train_transform)\n",
    "val_dataset = BratsDataset(val_paths, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5odi1Ug8Ho9"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders = {'train': train_loader,\n",
    "               'val': val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_loader))\n",
    "for i in range(len(sample[0])):\n",
    "    f, axes = plt.subplots(1, 2)\n",
    "    f.set_size_inches(12,12)\n",
    "    image = sample[0][i]\n",
    "    image = torch.tensor(image)\n",
    "    image = image.permute(1,2,0)\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('image')\n",
    "    mask = sample[1][i]\n",
    "    mask = torch.tensor(mask)\n",
    "    mask = mask.permute(1,2,0)\n",
    "    axes[1].set_title('mask')\n",
    "    axes[1].imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dkEL3x3VFqqw",
    "outputId": "a91f93d9-dd88-497c-df34-36e1c4fb15a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = next(iter(val_loader))\n",
    "for i in range(len(sample[0])):\n",
    "    f, axes = plt.subplots(1, 2)\n",
    "    f.set_size_inches(12,12)\n",
    "    image = sample[0][i]\n",
    "    image = torch.tensor(image)\n",
    "    image = image.permute(1,2,0)\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('image')\n",
    "    mask = sample[1][i]\n",
    "    mask = torch.tensor(mask)\n",
    "    mask = mask.permute(1,2,0)\n",
    "    axes[1].set_title('mask')\n",
    "    axes[1].imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pD2zoZRAQpS"
   },
   "source": [
    "# Train function, metrics, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nXpIeqaLIwR"
   },
   "outputs": [],
   "source": [
    "def dice_score(pred, target, smooth = 1e-6):\n",
    "    \"\"\" This definition generalize to real valued pred and target vector.\n",
    "        This should be differentiable.\n",
    "        pred: tensor with first dimension as batch\n",
    "        target: tensor with first dimension as batch\n",
    "    \"\"\"\n",
    "\n",
    "    # have to use contiguous since they may from a torch.view op\n",
    "    iflat = pred.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    A_sum = torch.sum(iflat * iflat)\n",
    "    B_sum = torch.sum(tflat * tflat)\n",
    "    \n",
    "    return (2. * intersection + smooth) / (A_sum + B_sum + smooth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqM-xoBkXBSU"
   },
   "outputs": [],
   "source": [
    "def calc_loss(pred, target, metrics, bce_weight=0.5, dice_weight=0.5, focal_weight=0):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_score(pred, target)\n",
    "    dice_loss = 1 - dice\n",
    "    focal_loss = FocalLoss(logits=True)\n",
    "\n",
    "    loss = bce * bce_weight + dice_loss * dice_weight + focal_weight * focal_loss(pred, target)\n",
    "\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler,\n",
    "                bce_weight=0.5, dice_weight=0.5, focal_weight=0,\n",
    "                num_epochs=25, isDeepLabV3=False):\n",
    "    loss_train = []\n",
    "    dice_train = []\n",
    "    loss_val = []\n",
    "    dice_val = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_dice = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(' Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 65)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if isDeepLabV3:\n",
    "                        outputs = model(inputs)['out']\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics, bce_weight, dice_weight, focal_weight)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "            scheduler.step()\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            epoch_dice = metrics['dice'] / epoch_samples\n",
    "            \n",
    "            if phase == 'train': \n",
    "                loss_train.append(epoch_loss)\n",
    "                dice_train.append(epoch_dice)\n",
    "            else:\n",
    "                loss_val.append(epoch_loss)\n",
    "                dice_val.append(epoch_dice)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_dice > best_dice:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_dice = epoch_dice\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val dice: {:4f}'.format(best_dice))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, loss_train, dice_train, loss_val, dice_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(*paths):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.grid(True)\n",
    "    for i in range(len(paths)):\n",
    "        with open(paths[i], \"r\") as f:\n",
    "            values = f.read()\n",
    "            values = values.split(\",\")\n",
    "            values = values[:-1]\n",
    "            values = list(map(lambda x: float(x), values))\n",
    "            f.close()\n",
    "        plt.plot(values, label=paths[i][38:-13])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masks(model, inputs, isDeepLabV3=False):\n",
    "    inputs = inputs.to(device)\n",
    "    if isDeepLabV3:\n",
    "        pred = model(inputs)['out']\n",
    "    else:\n",
    "        pred = model(inputs)\n",
    "    # The loss functions include the sigmoid function.\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = pred.data.cpu().numpy()\n",
    "    pred = torch.tensor(pred)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def plot_predictions(inputs, labels, pred):\n",
    "    for i in range(len(labels)):\n",
    "        f, axes = plt.subplots(1, 3)\n",
    "        f.set_size_inches(12,12)\n",
    "        image = inputs[i]\n",
    "        image = image.permute(1,2,0)\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title('MRI-photo')\n",
    "        real_mask = labels[i]\n",
    "        real_mask = real_mask.permute(1,2,0)\n",
    "        axes[1].imshow(real_mask)\n",
    "        axes[1].set_title('real_mask')\n",
    "        predicted_mask = pred[i]\n",
    "        predicted_mask = predicted_mask.permute(1,2,0)\n",
    "        predicted_mask.apply_(lambda x: 1 if x > 0.5 else 0)\n",
    "        #predicted_mask.apply_(lambda x: 0 if x < 0.1 else x)\n",
    "        axes[2].set_title('predicted_mask')\n",
    "        axes[2].imshow(predicted_mask)\n",
    "        \n",
    "def show_dices_fot_predict(labels, pred):\n",
    "    average_dice = 0\n",
    "    for i in range(len(labels)):\n",
    "        print(dice_score(labels[i], pred[i]))\n",
    "        average_dice += dice_score(labels[i], pred[i])\n",
    "    return average_dice/(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUeChuitArBt"
   },
   "source": [
    "# Unet with ResNet18-Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5yaKGW3_KFJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained=False)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Unet (with dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBCFQ7OPikMp"
   },
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "        \n",
    "start_fm = 16\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        \n",
    "        # Input 256x256x3\n",
    "        \n",
    "        #Contracting Path\n",
    "        \n",
    "        #(Double) Convolution 1        \n",
    "        self.double_conv1 = double_conv(3, start_fm, 3, 1, 1)\n",
    "        #Max Pooling 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 2\n",
    "        self.double_conv2 = double_conv(start_fm, start_fm * 2, 3, 1, 1)\n",
    "        #Max Pooling 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 3\n",
    "        self.double_conv3 = double_conv(start_fm * 2, start_fm * 4, 3, 1, 1)\n",
    "        #Max Pooling 3\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 4\n",
    "        self.double_conv4 = double_conv(start_fm * 4, start_fm * 8, 3, 1, 1)\n",
    "        #Max Pooling 4\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 5\n",
    "        self.double_conv5 = double_conv(start_fm * 8, start_fm * 16, 3, 1, 1)\n",
    "        \n",
    "        #Transposed Convolution 4\n",
    "        self.t_conv4 = nn.ConvTranspose2d(start_fm * 16, start_fm * 8, 2, 2)\n",
    "        # Expanding Path Convolution 4 \n",
    "        self.ex_double_conv4 = double_conv(start_fm * 16, start_fm * 8, 3, 1, 1)\n",
    "        \n",
    "        #Transposed Convolution 3\n",
    "        self.t_conv3 = nn.ConvTranspose2d(start_fm * 8, start_fm * 4, 2, 2)\n",
    "        #Convolution 3\n",
    "        self.ex_double_conv3 = double_conv(start_fm * 8, start_fm * 4, 3, 1, 1)\n",
    "        \n",
    "        #Transposed Convolution 2\n",
    "        self.t_conv2 = nn.ConvTranspose2d(start_fm * 4, start_fm * 2, 2, 2)\n",
    "        #Convolution 2\n",
    "        self.ex_double_conv2 = double_conv(start_fm * 4, start_fm * 2, 3, 1, 1)\n",
    "        \n",
    "        #Transposed Convolution 1\n",
    "        self.t_conv1 = nn.ConvTranspose2d(start_fm * 2, start_fm, 2, 2)\n",
    "        #Convolution 1\n",
    "        self.ex_double_conv1 = double_conv(start_fm * 2, start_fm, 3, 1, 1)\n",
    "        \n",
    "        # One by One Conv\n",
    "        self.one_by_one = nn.Conv2d(start_fm, 3, 1, 1, 0)\n",
    "        #self.final_act = nn.Sigmoid()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Contracting Path\n",
    "        conv1 = self.double_conv1(inputs)\n",
    "        maxpool1 = self.maxpool1(conv1)\n",
    "\n",
    "        conv2 = self.double_conv2(maxpool1)\n",
    "        maxpool2 = self.maxpool2(conv2)\n",
    "        \n",
    "        dropout1 = self.dropout(maxpool2)\n",
    "\n",
    "        conv3 = self.double_conv3(dropout1)\n",
    "        maxpool3 = self.maxpool3(conv3)\n",
    "\n",
    "        conv4 = self.double_conv4(maxpool3)\n",
    "        maxpool4 = self.maxpool4(conv4)\n",
    "        \n",
    "        dropout2 = self.dropout(maxpool4)\n",
    "            \n",
    "        # Bottom\n",
    "        conv5 = self.double_conv5(dropout2)\n",
    "        \n",
    "        # Expanding Path\n",
    "        t_conv4 = self.t_conv4(conv5)\n",
    "        cat4 = torch.cat([conv4 ,t_conv4], 1)\n",
    "        ex_conv4 = self.ex_double_conv4(cat4)\n",
    "        \n",
    "        t_conv3 = self.t_conv3(ex_conv4)\n",
    "        cat3 = torch.cat([conv3 ,t_conv3], 1)\n",
    "        ex_conv3 = self.ex_double_conv3(cat3)\n",
    "\n",
    "        t_conv2 = self.t_conv2(ex_conv3)\n",
    "        cat2 = torch.cat([conv2 ,t_conv2], 1)\n",
    "        ex_conv2 = self.ex_double_conv2(cat2)\n",
    "        \n",
    "        t_conv1 = self.t_conv1(ex_conv2)\n",
    "        cat1 = torch.cat([conv1 ,t_conv1], 1)\n",
    "        ex_conv1 = self.ex_double_conv1(cat1)\n",
    "        \n",
    "        one_by_one = self.one_by_one(ex_conv1)\n",
    "        \n",
    "        return one_by_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(model, model_name, bce_weight=0.5, \n",
    "                    dice_weight=0.5, focal_weight=0, \n",
    "                    num_epochs=25, isDeepLabV3=False):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device:', device)\n",
    "    print('Batch size:', batch_size)\n",
    "\n",
    "    optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.3)\n",
    "\n",
    "    model, loss_train, dice_train, loss_val, dice_val = train_model(model, optimizer_ft,\n",
    "                                                                    exp_lr_scheduler, num_epochs=num_epochs, \n",
    "                                                                    isDeepLabV3=isDeepLabV3,\n",
    "                                                                    bce_weight=bce_weight,\n",
    "                                                                    dice_weight=dice_weight,\n",
    "                                                                    focal_weight=focal_weight)\n",
    "\n",
    "    save_root = f'C:\\\\Users\\\\Admin\\\\Desktop\\\\KohanovDiploma\\\\models_v2\\\\{model_name}\\\\epochs_{num_epochs}\\\\bce{bce_weight}_dice{dice_weight}_focal{focal_weight}\\\\'\n",
    "    os.makedirs(save_root)\n",
    "    \n",
    "    with open(save_root + 'loss_train.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s,\" % value for value in loss_train)\n",
    "        filehandle.close()\n",
    "    with open(save_root + 'dice_train.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s,\" % value for value in dice_train)\n",
    "        filehandle.close()\n",
    "    with open(save_root + 'loss_val.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s,\" % value for value in loss_val)\n",
    "        filehandle.close()\n",
    "    with open(save_root + 'dice_val.txt', 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s,\" % value for value in dice_val)\n",
    "        filehandle.close()\n",
    "\n",
    "    torch.save(model.state_dict(),\n",
    "               save_root + model_name)\n",
    "    print('MODEL AND LOGS SAVED TO: ' + save_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements with DeepLabV3 (find the best loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.3, DICE_weight = 0.7, FOCAL_weight = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabv3_model = deeplabv3_resnet50(num_classes=3)\n",
    "deeplabv3_model = deeplabv3_model.to(device)\n",
    "model_name = 'deeplabv3'\n",
    "\n",
    "train_and_save(deeplabv3_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.7, focal_weight=0, \n",
    "               num_epochs=30, isDeepLabV3=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves('C:\\\\Users\\\\Admin\\\\Desktop\\\\KohanovDiploma\\\\models_v2\\\\unet_aug_affine\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\dice_val.txt',\n",
    "            'C:\\\\Users\\\\Admin\\\\Desktop\\\\KohanovDiploma\\\\models_v2\\\\unet_aug_soft\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\dice_val.txt',\n",
    "            'C:\\\\Users\\\\Admin\\\\Desktop\\\\KohanovDiploma\\\\models_v2\\\\unet_aug_strong\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\dice_val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.5, DICE_weight = 0.5, FOCAL_weight = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3_model = deeplabv3_resnet50(num_classes=3)\n",
    "deeplabv3_model = deeplabv3_model.to(device)\n",
    "model_name = 'deeplabv3'\n",
    "\n",
    "train_and_save(deeplabv3_model, model_name, bce_weight=0.5, \n",
    "               dice_weight=0.5, focal_weight=0, \n",
    "               num_epochs=10, isDeepLabV3=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.3, DICE_weight = 0.2, FOCAL_weight = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabv3_model = deeplabv3_resnet50(num_classes=3)\n",
    "deeplabv3_model = deeplabv3_model.to(device)\n",
    "model_name = 'deeplabv3'\n",
    "\n",
    "train_and_save(deeplabv3_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.2, focal_weight=0.5, \n",
    "               num_epochs=10, isDeepLabV3=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЛУЧШИЙ ЛОСС ДЛЯ DEEPLABV3 - bce03_dice02_focal05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unet_model = Unet().to(device)\n",
    "unet_model.load_state_dict(torch.load('C:\\\\Users\\\\Admin\\\\Desktop\\\\KohanovDiploma\\\\models_v2\\\\unet_aug_affine\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\unet_aug_affine'))\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(val_loader))\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "pred = predict_masks(unet_model, inputs, isDeepLabV3=False)\n",
    "\n",
    "inputs = inputs.data.cpu().numpy()\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = labels.to(device)\n",
    "labels = labels.data.cpu().numpy()\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "plot_predictions(inputs, labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_dice = show_dices_fot_predict(labels, pred)\n",
    "average_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements with Unet (find the best loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.3, DICE_weight = 0.7, FOCAL_weight = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = Unet().to(device)\n",
    "\n",
    "model_name = 'unet'\n",
    "\n",
    "train_and_save(unet_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.7, focal_weight=0, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.5, DICE_weight = 0.5, FOCAL_weight = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = Unet().to(device)\n",
    "\n",
    "model_name = 'unet'\n",
    "\n",
    "train_and_save(unet_model, model_name, bce_weight=0.5, \n",
    "               dice_weight=0.5, focal_weight=0, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.3, DICE_weight = 0.2, FOCAL_weight = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = Unet().to(device)\n",
    "\n",
    "model_name = 'unet'\n",
    "\n",
    "train_and_save(unet_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.2, focal_weight=0.5, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet losses on Validation (50 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_curves('models\\\\unet\\\\epochs_50\\\\bce0_dice1_focal0\\\\loss_val.txt',\n",
    "            'models\\\\unet\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\loss_val.txt',\n",
    "            'models\\\\unet\\\\epochs_50\\\\bce0.5_dice0.5_focal0\\\\loss_val.txt',\n",
    "            'models\\\\unet\\\\epochs_50\\\\bce0.3_dice0.2_focal0.5\\\\loss_val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet dices on validation (50 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_curves('models\\\\unet\\\\epochs_50\\\\bce0_dice1_focal0\\\\dice_val.txt',\n",
    "            'models\\\\unet\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\dice_val.txt',\n",
    "            'models\\\\unet\\\\epochs_50\\\\bce0.5_dice0.5_focal0\\\\dice_val.txt',\n",
    "            'models\\\\unet\\\\epochs_50\\\\bce0.3_dice0.2_focal0.5\\\\dice_val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЛУЧШИЙ ЛОСС ДЛЯ UNet - bce05_dice05_focal0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements with TernausNet (find the best loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.3, DICE_weight = 0.7, FOCAL_weight = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "num_class = 3\n",
    "resnet_unet_model = ResNetUNet(num_class).to(device)\n",
    "model_name = 'ternausnet'\n",
    "\n",
    "train_and_save(resnet_unet_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.7, focal_weight=0, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.5, DICE_weight = 0.5, FOCAL_weight = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "num_class = 3\n",
    "resnet_unet_model = ResNetUNet(num_class).to(device)\n",
    "model_name = 'ternausnet'\n",
    "\n",
    "train_and_save(resnet_unet_model, model_name, bce_weight=0.5, \n",
    "               dice_weight=0.5, focal_weight=0, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE_weight = 0.3, DICE_weight = 0.2, FOCAL_weight = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "num_class = 3\n",
    "resnet_unet_model = ResNetUNet(num_class).to(device)\n",
    "model_name = 'ternausnet'\n",
    "\n",
    "train_and_save(resnet_unet_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.2, focal_weight=0.5, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements with unet (augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256,256,cv2.INTER_NEAREST),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "    \n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256,256,cv2.INTER_NEAREST),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "train_dataset = BratsDataset(train_paths, train_transform)\n",
    "val_dataset = BratsDataset(val_paths, val_transform)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders = {'train': train_loader,\n",
    "               'val': val_loader}\n",
    "\n",
    "unet_model = Unet().to(device)\n",
    "\n",
    "model_name = 'unet_aug_affine'\n",
    "\n",
    "train_and_save(unet_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.7, focal_weight=0, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256,256,cv2.INTER_NEAREST),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomGamma(p=0.5),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256,256,cv2.INTER_NEAREST),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = BratsDataset(train_paths, train_transform)\n",
    "val_dataset = BratsDataset(val_paths, val_transform)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders = {'train': train_loader,\n",
    "               'val': val_loader}\n",
    "\n",
    "unet_model = Unet().to(device)\n",
    "\n",
    "model_name = 'unet_aug_soft'\n",
    "\n",
    "train_and_save(unet_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.7, focal_weight=0, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256,256,cv2.INTER_NEAREST),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.1, \n",
    "                       rotate_limit=10, p=0.3),\n",
    "    A.OneOf([\n",
    "        A.ElasticTransform(p=0.5, alpha=120, \n",
    "                         sigma=120*0.02, alpha_affine=120*0.03),\n",
    "        A.GridDistortion(p=0.3),\n",
    "        A.OpticalDistortion(p=0.3, distort_limit=2, shift_limit=0.2)\n",
    "    ], p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomGamma(p=0.5),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256,256,cv2.INTER_NEAREST),\n",
    "    ToTensorV2(transpose_mask=True)\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = BratsDataset(train_paths, train_transform)\n",
    "val_dataset = BratsDataset(val_paths, val_transform)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders = {'train': train_loader,\n",
    "               'val': val_loader}\n",
    "\n",
    "unet_model = Unet().to(device)\n",
    "\n",
    "model_name = 'unet_aug_strong'\n",
    "\n",
    "train_and_save(unet_model, model_name, bce_weight=0.3, \n",
    "               dice_weight=0.7, focal_weight=0, \n",
    "               num_epochs=50, isDeepLabV3=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves('models_v2\\\\unet_aug_soft\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\dice_val.txt',\n",
    "            'models_v2\\\\unet_aug_affine\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\dice_val.txt',\n",
    "            'models_v2\\\\unet_aug_strong\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\dice_val.txt',\n",
    "            'models_v2\\\\unet_with_dropout\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\dice_val.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшая аугментация - афинная (геометрическая)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet prtedictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unet_model = Unet().to(device)\n",
    "unet_model.load_state_dict(torch.load('models_v2\\\\unet_aug_affine\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\unet_aug_affine'))\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(val_loader))\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "pred = predict_masks(unet_model, inputs, isDeepLabV3=False)\n",
    "\n",
    "inputs = inputs.data.cpu().numpy()\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = labels.to(device)\n",
    "labels = labels.data.cpu().numpy()\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "plot_predictions(inputs, labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_dice = show_dices_fot_predict(labels, pred)\n",
    "average_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unet_model = Unet().to(device)\n",
    "unet_model.load_state_dict(torch.load('models_v2\\\\unet_with_dropout\\\\epochs_50\\\\bce0.3_dice0.7_focal0\\\\unet_with_dropout'))\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "deeplabv3_model = deeplabv3_resnet50(num_classes=3)\n",
    "deeplabv3_model = deeplabv3_model.to(device)\n",
    "deeplabv3_model.load_state_dict(torch.load('models\\\\deeplabv3\\\\epochs_30\\\\bce0.5_dice0.5_focal0\\\\deeplabv3'))\n",
    "deeplabv3_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 3\n",
    "ternaus_model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "ternaus_model.load_state_dict(torch.load('models_v2\\\\ternausnet\\\\epochs_20\\\\bce0.3_dice0.7_focal0\\\\ternausnet'))\n",
    "ternaus_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(val_loader))\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "inputs = inputs.data.cpu().numpy()\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = labels.to(device)\n",
    "labels = labels.data.cpu().numpy()\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "f, axes = plt.subplots(1, 5)\n",
    "f.set_size_inches(100,100)\n",
    "image = inputs[1]\n",
    "image = image.permute(1,2,0)\n",
    "axes[0].imshow(image)\n",
    "real_mask = labels[1]\n",
    "real_mask = real_mask.permute(1,2,0)\n",
    "axes[1].imshow(real_mask)\n",
    "\n",
    "pred_unet = predict_masks(unet_model, inputs, isDeepLabV3=False)\n",
    "unet_mask = pred_unet[1]\n",
    "unet_mask = unet_mask.permute(1,2,0)\n",
    "unet_mask.apply_(lambda x: 1 if x > 0.5 else 0)\n",
    "axes[2].imshow(unet_mask)\n",
    "\n",
    "pred_deeplab = predict_masks(deeplabv3_model, inputs, isDeepLabV3=True)\n",
    "deeplab_mask = pred_deeplab[1]\n",
    "deeplab_mask = deeplab_mask.permute(1,2,0)\n",
    "deeplab_mask.apply_(lambda x: 1 if x > 0.5 else 0)\n",
    "axes[3].imshow(deeplab_mask)\n",
    "\n",
    "pred_ternaus = predict_masks(ternaus_model, inputs, isDeepLabV3=False)\n",
    "ternaus_mask = pred_ternaus[1]\n",
    "ternaus_mask = ternaus_mask.permute(1,2,0)\n",
    "ternaus_mask.apply_(lambda x: 1 if x > 0.5 else 0)\n",
    "axes[4].imshow(ternaus_mask)\n",
    "\n",
    "# pred_pix2pix = cv2.imread('/home/podaval/Kochanov_diploma/pytorch-CycleGAN-and-pix2pix/results/brats_pix2pix/test_latest/images/56_fake_B.png')\n",
    "# axes[5].imshow(pred_pix2pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(val_loader))\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "inputs = inputs.data.cpu().numpy()\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = labels.to(device)\n",
    "labels = labels.data.cpu().numpy()\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "f, axes = plt.subplots(1, 5)\n",
    "f.set_size_inches(100,100)\n",
    "image = inputs[1]\n",
    "image = image.permute(1,2,0)\n",
    "axes[0].imshow(image)\n",
    "\n",
    "real_mask = labels[1]\n",
    "real_mask = real_mask.permute(1,2,0)\n",
    "axes[1].imshow(real_mask)\n",
    "\n",
    "pred_unet = predict_masks(unet_model, inputs, isDeepLabV3=False)\n",
    "unet_mask = pred_unet[1]\n",
    "unet_mask = unet_mask.permute(1,2,0)\n",
    "unet_mask.apply_(lambda x: 1 if x > 0.5 else 0)\n",
    "axes[2].imshow(unet_mask)\n",
    "\n",
    "pred_deeplab = predict_masks(deeplabv3_model, inputs, isDeepLabV3=True)\n",
    "deeplab_mask = pred_deeplab[1]\n",
    "deeplab_mask = deeplab_mask.permute(1,2,0)\n",
    "deeplab_mask.apply_(lambda x: 1 if x > 0.5 else 0)\n",
    "axes[3].imshow(deeplab_mask)\n",
    "\n",
    "pred_ternaus = predict_masks(ternaus_model, inputs, isDeepLabV3=False)\n",
    "ternaus_mask = pred_ternaus[1]\n",
    "ternaus_mask = ternaus_mask.permute(1,2,0)\n",
    "ternaus_mask.apply_(lambda x: 1 if x > 0.5 else 0)\n",
    "axes[4].imshow(ternaus_mask)\n",
    "\n",
    "# pred_pix2pix = cv2.imread('/home/podaval/Kochanov_diploma/pytorch-CycleGAN-and-pix2pix/results/brats_pix2pix/test_latest/images/56_fake_B.png')\n",
    "# axes[5].imshow(pred_pix2pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tUkQOObsAmmM"
   ],
   "name": "Brain_Tumor_Segmantation_diploma.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
